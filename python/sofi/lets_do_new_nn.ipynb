{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import keras_cv\n",
    "from keras_cv import bounding_box\n",
    "from keras_cv import visualization\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_annotation(txt_file):\n",
    "\n",
    "    image_path = txt_file.split(\"/\")\n",
    "    image_path[-2] = \"imgs\"\n",
    "    image_path[-1] = image_path[-1][:-4] + \".jpg\"\n",
    "    image_path = \"/\".join(image_path)\n",
    "\n",
    "    boxes = []\n",
    "    class_ids = []\n",
    "    with open(txt_file) as file:\n",
    "\n",
    "        try:\n",
    "            for line in file:\n",
    "                lineSplit = line.split()\n",
    "                box = lineSplit[1:]\n",
    "                box = [640*float(coor) for coor in box]\n",
    "                # correct from center to edge of the box\n",
    "                box[0] -= box[2]/2\n",
    "                box[1] -= box[3]/2\n",
    "\n",
    "                boxes.append(box)\n",
    "\n",
    "\n",
    "                class_ids.append(int(lineSplit[0]))\n",
    "        except:\n",
    "            print(f\"Filename has something wrong {txt_file}\")\n",
    "\n",
    "    return image_path, boxes, class_ids\n",
    "\n",
    "\n",
    "def load_image(image_path):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)  # Decode as a 3-channel RGB image\n",
    "    image = tf.image.resize(image, (640, 640))  # Resize image to a fixed size\n",
    "    image = tf.cast(image, tf.float32)  # Convert to float32 for model input\n",
    "    return image\n",
    "\n",
    "# def load_image(image_path):\n",
    "#     image = tf.io.read_file(image_path)\n",
    "#     image = tf.image.decode_png(image, channels=3)\n",
    "#     return image\n",
    "\n",
    "def load_dataset(image_path, classes, bbox):\n",
    "\n",
    "    image = load_image(image_path)\n",
    "    bounding_boxes = {\n",
    "        \"classes\": tf.cast(classes, dtype=tf.float32),\n",
    "        \"boxes\": bbox,\n",
    "    }\n",
    "    return {\"images\": tf.cast(image, tf.float32), \"bounding_boxes\": bounding_boxes}\n",
    "\n",
    "\n",
    "def visualize_dataset(inputs, value_range, rows, cols, bounding_box_format, class_mapping):\n",
    "\n",
    "    inputs = next(iter(inputs.take(1)))\n",
    "    images, bounding_boxes = inputs[\"images\"], inputs[\"bounding_boxes\"]\n",
    "    visualization.plot_bounding_box_gallery(\n",
    "        images,\n",
    "        value_range=value_range,\n",
    "        rows=rows,\n",
    "        cols=cols,\n",
    "        y_true=bounding_boxes,\n",
    "        scale=5,\n",
    "        font_scale=0.7,\n",
    "        bounding_box_format=bounding_box_format,\n",
    "        class_mapping=class_mapping,\n",
    "    )\n",
    "\n",
    "   \n",
    "class EvaluateCOCOMetricsCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, data, save_dir, bounding_box_format):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.metrics = keras_cv.metrics.BoxCOCOMetrics(\n",
    "            bounding_box_format=bounding_box_format,\n",
    "            evaluate_freq=1e9,\n",
    "        )\n",
    "\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        self.metrics.reset_state()\n",
    "        for batch in self.data:\n",
    "            images, y_true = batch[0], batch[1]\n",
    "            y_pred = self.model.predict(images, verbose=0)\n",
    "            self.metrics.update_state(y_true, y_pred)\n",
    "\n",
    "        metrics = self.metrics.result(force=True)\n",
    "        logs.update(metrics)\n",
    "\n",
    "        # save logs\n",
    "        with open(os.path.join(self.save_dir, f\"epoch_{epoch}_logs.txt\"), \"w\") as file:\n",
    "            for key, value in logs.items():\n",
    "\n",
    "                if isinstance(value, tf.Tensor):\n",
    "                    file.write(key + \":\" + str(value.numpy()) + \"\\n\")\n",
    "                else:\n",
    "                    file.write(key + \":\" + str(value) + \"\\n\")\n",
    "\n",
    "\n",
    "        # save model\n",
    "        self.model.save(os.path.join(self.save_dir, f\"epoch_{epoch}_model.keras\"))\n",
    "\n",
    "        return logs\n",
    "\n",
    "def evaluateCocoMetrics(model, data, bounding_box_format):\n",
    "\n",
    "    boxCOCOMetrics = keras_cv.metrics.BoxCOCOMetrics(\n",
    "        bounding_box_format=bounding_box_format,\n",
    "        evaluate_freq=1e9,\n",
    "    )\n",
    "\n",
    "    boxCOCOMetrics.reset_state()\n",
    "\n",
    "    numOfSteps = tf.data.experimental.cardinality(data).numpy()\n",
    "\n",
    "    for i,batch in enumerate(data):\n",
    "        print(f\"\\r Processing step {i+1}/{numOfSteps}\", end=\"\")\n",
    "        images, y_true = batch[0], batch[1]\n",
    "        y_pred = model.predict(images, verbose=0)\n",
    "        boxCOCOMetrics.update_state(y_true, y_pred)\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "    logs = boxCOCOMetrics.result(force=True)\n",
    "\n",
    "    return logs\n",
    "\n",
    "def visualize_detections(model, dataset, bounding_box_format, class_mapping):\n",
    "    images, y_true = next(iter(dataset.take(1)))\n",
    "    y_pred = model.predict(images)\n",
    "    y_pred = bounding_box.to_ragged(y_pred)\n",
    "    visualization.plot_bounding_box_gallery(\n",
    "        images,\n",
    "        value_range=(0, 255),\n",
    "        bounding_box_format=bounding_box_format,\n",
    "        y_true=y_true,\n",
    "        y_pred=y_pred,\n",
    "        scale=4,\n",
    "        rows=2,\n",
    "        cols=2,\n",
    "        show=True,\n",
    "        font_scale=0.7,\n",
    "        class_mapping=class_mapping,\n",
    "    )\n",
    "\n",
    "def visualize_batch(images, bounding_boxes, bbox_format=\"xywh\"):\n",
    "    \"\"\"\n",
    "    Visualizes a batch of images with their corresponding bounding boxes and class labels.\n",
    "\n",
    "    Parameters:\n",
    "    - images (Tensor): Batch of images.\n",
    "    - bounding_boxes (dict): Dictionary containing 'classes' and 'boxes'.\n",
    "    - bbox_format (str): Format of the bounding boxes ('xyxy' or 'xywh').\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    batch_size = images.shape[0]\n",
    "    for i in range(batch_size):\n",
    "        image = images[i].numpy().astype(np.uint8)\n",
    "        classes = bounding_boxes['classes'][i].numpy()\n",
    "        boxes = bounding_boxes['boxes'][i].numpy()\n",
    "        \n",
    "        # If 'boxes' is a RaggedTensor, convert to dense\n",
    "        if isinstance(boxes, tf.RaggedTensor):\n",
    "            boxes = boxes.to_tensor()\n",
    "        \n",
    "        # Plot the image\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.imshow(image)\n",
    "        ax = plt.gca()\n",
    "        \n",
    "        # Plot each bounding box with its class label\n",
    "        for j, box in enumerate(boxes):\n",
    "            if bbox_format == \"xyxy\":\n",
    "                x_min, y_min, x_max, y_max = box\n",
    "            elif bbox_format == \"xywh\":\n",
    "                x_center, y_center, width, height = box\n",
    "                x_min = x_center - (width / 2)\n",
    "                y_min = y_center - (height / 2)\n",
    "                x_max = x_center + (width / 2)\n",
    "                y_max = y_center + (height / 2)\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported bounding box format.\")\n",
    "            \n",
    "            # Draw rectangle\n",
    "            rect = plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min,\n",
    "                                 linewidth=2, edgecolor='r', facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "            \n",
    "            # Add class label text\n",
    "            class_label = int(classes[j])\n",
    "            plt.text(x_min, y_min - 10, str(class_label), \n",
    "                     fontsize=12, color='yellow', backgroundcolor='red')\n",
    "        \n",
    "        plt.title(f\"Batch Image {i+1}\")\n",
    "        plt.axis('off')\n",
    "        #plt.savefig(f\"/Users/vdk/Software/my_code/python/sofi/batch_error_demonstration/batch_image_{i+1}.png\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants and paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_RATIO = 0.1\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 0.001\n",
    "GLOBAL_CLIPNORM = 10.0\n",
    "\n",
    "BEST_EPOCH = 5\n",
    "\n",
    "bounding_box_format = \"xywh\"\n",
    "\n",
    "dataset_dir = \"/Users/vdk/Software/my_code/python/sofi/data/rad_photo_custom/yolo_labeled_files\"\n",
    "model_store_dir = \"/Users/vdk/Software/my_code/python/sofi/data/output\"\n",
    "\n",
    "class_ids = [\n",
    "    \"With_Radiation_sign\",\n",
    "    \"Without_Radiation_sign\",\n",
    "    \"Radiation_sign\"\n",
    "]\n",
    "class_mapping = dict(zip(range(len(class_ids)), class_ids))\n",
    "name2classID = {value:key for key,value in class_mapping.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload of the dataset from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_files = []\n",
    "image_paths_raw = []\n",
    "bbox_raw = []\n",
    "classes_raw = []\n",
    "\n",
    "for r, _, f in os.walk(dataset_dir):\n",
    "    for txt_filename in f:\n",
    "        txt_files.append(os.path.join(r, txt_filename))\n",
    "\n",
    "txt_files = sorted(txt_files)\n",
    "random.seed(10)\n",
    "random.shuffle(txt_files)\n",
    "\n",
    "for txt_file in tqdm(txt_files):\n",
    "    image_path, boxes, class_ids = parse_annotation(txt_file)\n",
    "    image_paths_raw.append(image_path)\n",
    "    bbox_raw.append(boxes)\n",
    "    classes_raw.append(class_ids)\n",
    "\n",
    "print(\"Lenght of imgaes list\",  len(image_paths_raw))\n",
    "print(\"Lenght of bbox list\",    len(bbox_raw))\n",
    "print(\"Lenght of classes list\", len(classes_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the dataset into 3 groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox = tf.ragged.constant(bbox_raw)\n",
    "classes = tf.ragged.constant(classes_raw)\n",
    "image_paths = tf.ragged.constant(image_paths_raw)\n",
    "\n",
    "data = tf.data.Dataset.from_tensor_slices((image_paths, classes, bbox))\n",
    "\n",
    "# Determine the number of validation samples\n",
    "num_valtest = int(len(txt_files) * SPLIT_RATIO)\n",
    "\n",
    "# Split the dataset into train and validation sets\n",
    "valtest_data = data.take(num_valtest+num_valtest)\n",
    "\n",
    "val_data = valtest_data.take(num_valtest)\n",
    "test_data = valtest_data.skip(num_valtest)\n",
    "\n",
    "train_data = data.skip(num_valtest+num_valtest)\n",
    "\n",
    "print(f\"Validation set size: {tf.data.experimental.cardinality(val_data).numpy()}\")\n",
    "print(f\"Test set size: {tf.data.experimental.cardinality(test_data).numpy()}\")\n",
    "print(f\"Training set size: {tf.data.experimental.cardinality(train_data).numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Data load, preprocess, shuffle and augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmenter = keras.Sequential(\n",
    "    layers=[\n",
    "        keras_cv.layers.RandomFlip(mode=\"horizontal\", bounding_box_format=bounding_box_format),\n",
    "        keras_cv.layers.JitteredResize(\n",
    "            target_size=(640, 640), scale_factor=(1, 1), bounding_box_format=bounding_box_format\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_ds = train_data.map(load_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_ds = train_ds.shuffle(BATCH_SIZE * 4)\n",
    "train_ds = train_ds.ragged_batch(BATCH_SIZE, drop_remainder=False)\n",
    "train_ds = train_ds.map(augmenter, num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_ds.take(1):\n",
    "    visualize_batch(batch['images'], batch['bounding_boxes'], bbox_format=bounding_box_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cardinality = tf.data.experimental.cardinality(train_ds).numpy()\n",
    "print(f\"Number of batches in train_ds: {cardinality}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Data load, preprocess, shuffle and augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resizing = keras_cv.layers.JitteredResize(\n",
    "    target_size=(640, 640),\n",
    "    scale_factor=(1, 1),\n",
    "    bounding_box_format=\"xywh\",\n",
    ")\n",
    "\n",
    "val_ds = val_data.map(load_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.shuffle(BATCH_SIZE * 4)\n",
    "val_ds = val_ds.ragged_batch(BATCH_SIZE, drop_remainder=False)\n",
    "val_ds = val_ds.map(resizing, num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation of the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize_dataset(\n",
    "#     train_ds, bounding_box_format=\"xywh\", value_range=(0, 255), rows=2, cols=2, class_mapping=class_mapping\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize_dataset(\n",
    "#     val_ds, bounding_box_format=\"xywh\", value_range=(0, 255), rows=2, cols=2, class_mapping=class_mapping\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up of the image detection neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_tuple(inputs):\n",
    "    return inputs[\"images\"], bounding_box.to_dense(inputs[\"bounding_boxes\"], max_boxes=32)\n",
    "\n",
    "train_ds = train_ds.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_ds = val_ds.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "yolo = keras_cv.models.YOLOV8Detector.from_preset(\n",
    "    \"mobilenet_v3_small_imagenet\",\n",
    "    bounding_box_format=\"xywh\",\n",
    "    num_classes=len(class_mapping),\n",
    "    input_shape = (640, 640, 3),\n",
    "    load_weights=False)\n",
    "\n",
    "# Freezing layers is common when leveraging pre-trained models to prevent overfitting and reduce training time.\n",
    "# Implication: When applied, the model’s layers will have their weights fixed and won’t be updated during backpropagation.\n",
    "isLayersTrainable = True \n",
    "\n",
    "for layer in yolo.layers:\n",
    "\n",
    "    if layer.name == \"tf.concat_5\" and not isLayersTrainable:\n",
    "        isLayersTrainable = True\n",
    "\n",
    "    if isLayersTrainable:\n",
    "        layer.trainable=True\n",
    "    else:\n",
    "        layer.trainable=False\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    global_clipnorm=GLOBAL_CLIPNORM,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_EPOCHS = 1\n",
    "\n",
    "yolo.compile(\n",
    "    optimizer=optimizer, classification_loss=\"binary_crossentropy\", box_loss=\"ciou\"\n",
    ")\n",
    "\n",
    "history = yolo.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=NUMBER_OF_EPOCHS,\n",
    "    callbacks=[EvaluateCOCOMetricsCallback(val_ds, model_store_dir, bounding_box_format)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filenames = os.listdir(model_store_dir)\n",
    "filenames = [filename for filename in filenames if \".txt\" in filename]\n",
    "\n",
    "logs = {}\n",
    "\n",
    "with open(os.path.join(model_store_dir, filenames[0]), \"r\") as file:\n",
    "\n",
    "    for line in file:\n",
    "        key = line.split(\":\")[0]\n",
    "\n",
    "        logs[key] = []\n",
    "\n",
    "for epoch in range(len(filenames)):\n",
    "\n",
    "    filename = f\"epoch_{epoch}_logs.txt\"\n",
    "\n",
    "    with open(os.path.join(model_store_dir, filename), \"r\") as file:\n",
    "\n",
    "        for line in file:\n",
    "\n",
    "            key, value = line.split(\":\")\n",
    "            value = float(value)\n",
    "\n",
    "            logs[key].append(value)\n",
    "\n",
    "epochs  = [i+1 for i in range(len(filenames))]\n",
    "\n",
    "for key, value in logs.items():\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(epochs[3:], value[3:])\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(key)\n",
    "\n",
    "\n",
    "test_ds = test_data.map(load_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test_ds = test_ds.shuffle(BATCH_SIZE * 4)\n",
    "test_ds = test_ds.ragged_batch(BATCH_SIZE, drop_remainder=True)\n",
    "test_ds = test_ds.map(resizing, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "test_ds = test_ds.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test_ds = test_ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "\n",
    "model_path = os.path.join(model_store_dir, f\"epoch_{BEST_EPOCH}_model.keras\")\n",
    "yolo_best = tf.keras.models.load_model(model_path)\n",
    "\n",
    "yolo_best.compile(\n",
    "    optimizer=optimizer, classification_loss=\"binary_crossentropy\", box_loss=\"ciou\"\n",
    ")\n",
    "\n",
    "logs = yolo_best.evaluate(test_ds)\n",
    "logs = {\"loss\": logs[0],\n",
    "        \"box_loss\": logs[1],\n",
    "        \"class_loss\": logs[2]\n",
    "}\n",
    "\n",
    "logs.update( evaluateCocoMetrics(yolo_best, test_ds, bounding_box_format) )\n",
    "\n",
    "print(\"{:<27} {:<10}\".format('Metric','Value'))\n",
    "\n",
    "for key, value in logs.items():\n",
    "    if isinstance(value, tf.Tensor):\n",
    "        print(\"{:<27} {:<10}\".format(key, value.numpy()))\n",
    "    else:\n",
    "        print(\"{:<27} {:<10}\".format(key, value))\n",
    "\n",
    "\n",
    "visualize_detections(yolo_best, dataset=val_ds, bounding_box_format=\"xywh\", class_mapping=class_mapping) \n",
    "\n",
    "train_image_paths = train_data.map(lambda image_path, _, __: image_path)\n",
    "datasetFileTrain = os.path.join(model_store_dir, \"trainDatasetFile.txt\")\n",
    "\n",
    "with open(datasetFileTrain, \"w\") as trainDataFile:\n",
    "    for image_path in train_image_paths:\n",
    "            trainDataFile.write(image_path.numpy().decode('utf-8') + \"\\n\")\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "val_image_paths = val_data.map(lambda image_path, _, __: image_path)\n",
    "datasetFileVal = os.path.join(model_store_dir, \"valDatasetFile.txt\")\n",
    "\n",
    "with open(datasetFileVal, \"w\") as valDataFile:\n",
    "    for image_path in val_image_paths:\n",
    "            valDataFile.write(image_path.numpy().decode('utf-8') + \"\\n\")\n",
    "\n",
    "#########################################################################\n",
    "\n",
    "test_image_paths = test_data.map(lambda image_path, _, __: image_path)\n",
    "datasetFileTest = os.path.join(model_store_dir, \"testDatasetFile.txt\")\n",
    "\n",
    "with open(datasetFileTest, \"w\") as testDataFile:\n",
    "    for image_path in test_image_paths:\n",
    "            testDataFile.write(image_path.numpy().decode('utf-8') + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
